{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['股票', '房产', '时尚', '体育', '教育', '社会', '家居', '游戏', '科技', '娱乐', '财经', '时政', '星座', '彩票']\n",
      "['/home/mis/courses/nlp/THUCNews/股票/740647.txt', '/home/mis/courses/nlp/THUCNews/股票/661867.txt', '/home/mis/courses/nlp/THUCNews/股票/647163.txt', '/home/mis/courses/nlp/THUCNews/股票/665074.txt', '/home/mis/courses/nlp/THUCNews/股票/661205.txt', '/home/mis/courses/nlp/THUCNews/股票/682079.txt', '/home/mis/courses/nlp/THUCNews/股票/688286.txt', '/home/mis/courses/nlp/THUCNews/股票/717713.txt', '/home/mis/courses/nlp/THUCNews/股票/684286.txt', '/home/mis/courses/nlp/THUCNews/股票/680461.txt']\n"
     ]
    }
   ],
   "source": [
    "corpus_path=\"/home/mis/courses/nlp/THUCNews\"\n",
    "category_list = os.listdir(corpus_path)\n",
    "print(category_list)\n",
    "file_list=[]\n",
    "for cl in category_list:\n",
    "    category_path=os.path.join(corpus_path,cl)\n",
    "    category_file_list=os.listdir(category_path)\n",
    "    for file_name in category_file_list[0:1000]:\n",
    "        file_list.append(os.path.join(category_path,file_name))\n",
    "print(file_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _word_ngrams(tokens, stop_words=None,ngram_range=(1,1)):\n",
    "        \"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        # handle token n-grams\n",
    "        min_n, max_n = ngram_range\n",
    "        if max_n != 1:\n",
    "            original_tokens = tokens\n",
    "            tokens = []\n",
    "            n_original_tokens = len(original_tokens)\n",
    "            for n in range(min_n,\n",
    "                            min(max_n + 1, n_original_tokens + 1)):\n",
    "                for i in range(n_original_tokens - n + 1):\n",
    "                    tokens.append(\" \".join(original_tokens[i: i + n]))\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_count(file_path,counter,n_gram):\n",
    "    f=open(file_path,'r')\n",
    "    lines=f.read()\n",
    "    lines.strip('\\n')\n",
    "    lines = re.sub(\"[A-Za-z0-9\\：\\·\\—\\n \\% \\“ \\” \\( \\) \\u3000]\", \"\", lines)\n",
    "    #print(lines)\n",
    "    f.close()\n",
    "    seg_list = list(jieba.cut(lines, cut_all=False))\n",
    "    gramWords = _word_ngrams(tokens = seg_list,ngram_range=(n_gram,n_gram))\n",
    "    #print(gramWords)\n",
    "    for x in gramWords:\n",
    "        if len(x)>1 and x != '\\r\\n':\n",
    "            counter[x] += 1\n",
    "    #for (k,v) in c.most_common(2):# 输出词频最高的前两个词\n",
    "        #print(\"%s:%d\"%(k,v))\n",
    "    #print(counter.items())\n",
    "    return len(gramWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_entropy(counter,words_count,n_gram):\n",
    "    c_dict=dict(counter.items())\n",
    "    for k,v in c_dict.items():\n",
    "        v=int(v)/words_count\n",
    "        c_dict[k]=v\n",
    "    #print(c_dict)\n",
    "    entropy={}\n",
    "    for k,v in c_dict.items():\n",
    "        entropy[k]=-v*math.log(v)\n",
    "    #print(entropy)\n",
    "    entropy_sum=0\n",
    "    for k,v in entropy.items():\n",
    "        entropy_sum+=v\n",
    "    return entropy_sum/n_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.506 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "c=Counter()\n",
    "word_sum=0\n",
    "for file in file_list:\n",
    "    word_sum+=get_file_count(file,c,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6852188\n"
     ]
    }
   ],
   "source": [
    "print(word_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8355579241003035\n"
     ]
    }
   ],
   "source": [
    "entropy=cal_entropy(c,word_sum,1)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=Counter()\n",
    "word_sum=0\n",
    "for file in file_list:\n",
    "    word_sum+=get_file_count(file,c,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6838188\n"
     ]
    }
   ],
   "source": [
    "print(word_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.510605408646749\n"
     ]
    }
   ],
   "source": [
    "entropy=cal_entropy(c,word_sum,2)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=Counter()\n",
    "word_sum=0\n",
    "for file in file_list:\n",
    "    word_sum+=get_file_count(file,c,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6824188\n"
     ]
    }
   ],
   "source": [
    "print(word_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.988542172345661\n"
     ]
    }
   ],
   "source": [
    "entropy=cal_entropy(c,word_sum,3)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
