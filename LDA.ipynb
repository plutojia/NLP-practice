{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mis/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "/home/mis/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/mis/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib  \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "data_train_path='GODJIA/datasets/20news-bydate/20news-bydate-train'\n",
    "data_test_path='GODJIA/datasets/20news-bydate/20news-bydate-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练文档总数： 11314\n",
      "训练标签总数： 11314\n"
     ]
    }
   ],
   "source": [
    "#将所有文件名读入\n",
    "train_doc_names=[]\n",
    "train_datas=[]\n",
    "label_names=os.listdir(data_train_path)\n",
    "train_label=[]\n",
    "for i,label_name in enumerate(label_names):\n",
    "    label_path=os.path.join(data_train_path,label_name)\n",
    "    label_doc_names=os.listdir(label_path)\n",
    "    train_label.extend([i]*len(label_doc_names))\n",
    "    for label_doc_name in label_doc_names:\n",
    "        label_doc_path=os.path.join(label_path,label_doc_name)\n",
    "        train_doc_names.append(label_doc_path)\n",
    "        with open(label_doc_path,'r',encoding=\"ISO-8859-1\") as f:\n",
    "            train_datas.append(f.read())\n",
    "            f.close()\n",
    "            \n",
    "                \n",
    "            \n",
    "#print(train_doc_names)\n",
    "print('训练文档总数：',len(train_datas))\n",
    "print('训练标签总数：',len(train_label))\n",
    "#print(train_datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "300\n",
      "600\n",
      "900\n",
      "1200\n",
      "1500\n",
      "1800\n",
      "2100\n",
      "2400\n",
      "2700\n",
      "3000\n",
      "3300\n",
      "3600\n",
      "3900\n",
      "4200\n",
      "4500\n",
      "4800\n",
      "5100\n",
      "5400\n",
      "5700\n",
      "6000\n",
      "6300\n",
      "6600\n",
      "6900\n",
      "7200\n",
      "7500\n",
      "7800\n",
      "8100\n",
      "8400\n",
      "8700\n",
      "9000\n",
      "9300\n",
      "9600\n",
      "9900\n",
      "10200\n",
      "10500\n",
      "10800\n",
      "11100\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "textPre_FilePath='textPre.txt'\n",
    "def textPrecessing(text):\n",
    "    #小写化\n",
    "    text = text.lower()\n",
    "    #去除特殊标点\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, ' ')\n",
    "    #分词\n",
    "    wordLst = nltk.word_tokenize(text)\n",
    "    #去除停用词\n",
    "    filtered = [w for w in wordLst if w not in stopwords.words('english')]\n",
    "    #仅保留名词或特定POS   \n",
    "    refiltered =nltk.pos_tag(filtered)\n",
    "    filtered = [w for w, pos in refiltered if pos.startswith('NN')]\n",
    "    #词干化\n",
    "    ps = PorterStemmer()\n",
    "    filtered = [ps.stem(w) for w in filtered]\n",
    "\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "#train_datas_clean = [textPrecessing(doc) for doc in train_datas]\n",
    "\n",
    "train_datas_clean = []\n",
    "for i,desc in enumerate(train_datas) :\n",
    "    train_datas_clean.append(textPrecessing(desc))\n",
    "    if i%300==0:\n",
    "        print(i)\n",
    "\n",
    "with open(textPre_FilePath, 'w') as f:\n",
    "    for line in train_datas_clean:\n",
    "        f.write(line+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articl geneva rutger hopper virginia edu lawrenc other letter doctrin hell noth preacher man god hell place need die god fact thing infinit die centuri concept centuri writer cours termonolog god death scriptur word see signifi death separ ceastat reason agoni cross time etern member godhead met ladi model saint year distanc form death wait home bid death separ death separ death separ fellowship outer dark go jesu hell respect man promis wise man older children guid life cours love harsh god danger choos rule conscienc matter wit thing caus fear jesu doubt pinprick man dealt error knowledg order correct faith faith knowledg jesu pinprick life scriptur heb priest sympath thing sin kenosi passag state attribut human medit mine thesi conscious christ peopl studi concluss jesu member triniti part human race knowledg grasp omnisci understand extent jesu man jesu women tens situat surpris event place time histori time discuss curcuit breaker brain info freethink organ use info organ duction member club thinker contrast cours believ repositori everyth find stanc humanist wish buddhist marxist right doubt fact faith doubt thinker doubt critic humanist faith dout faith choos doubt faith argument speak fact skeptic skeptic evid doubt anyth someth person faith faith fact measur poverti term term peopl skeptic campu ministri time argument thought word point logic moment commun speak skeptic insist sens eloqu testimoni assumpt skeptic skeptic time room skeptic person skeptic stress belabor underlin know objectiv skeptic impossibl impass way presupposit proce pretent doubt doubt idea knowledg word myth presupposit doubt anyth someth doubt assumpt presuppost basi assumpt presuppostiton partner silenc absenc tell god worship burn hell cours statement milton millenia highland movi presupt faulti knowledg charact god premis theolog histori prophet word son reject god reject\n"
     ]
    }
   ],
   "source": [
    "textPre_FilePath='textPre.txt'\n",
    "train_datas_clean = []\n",
    "with open(textPre_FilePath, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line != '':\n",
    "            #train_datas_clean.append(line.strip())\n",
    "            train_datas_clean.append(' '.join(line.strip().split()[7:-1]))\n",
    "print(train_datas_clean[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf.model']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ModelPath='tf.model'\n",
    "n_features=3500\n",
    "#构建词汇统计向量并保存，仅运行首次\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(train_datas_clean)\n",
    "joblib.dump(tf_vectorizer,tf_ModelPath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ModelPath='tf.model'\n",
    "#得到存储的tf_vectorizer,节省预处理时间\n",
    "tf_vectorizer = joblib.load(tf_ModelPath)\n",
    "tf = tf_vectorizer.transform(train_datas_clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lda.model']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_path='lda.model'\n",
    "lda = LatentDirichletAllocation(n_components=40, \n",
    "                                max_iter=50,\n",
    "                                learning_method='batch')\n",
    "lda.fit(tf) #tf即为Document_word Sparse Matrix  \n",
    "\n",
    "#模型的保存/ 加载\n",
    "joblib.dump(lda,lda_path )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印每个主题下权重较高的term:\n",
      "Topic #0:\n",
      "edu articl univers organ cs host cc news write washington cso depart scienc comput stanford anyon mail cwru illinoi distribut\n",
      "Topic #1:\n",
      "file window program problem use font error tape manag organ ms directori charact copi work version time sy disk util\n",
      "Topic #2:\n",
      "ac colorado ohio state event window magnu univers articl boulder organ valu time widget host way problem spot cs manag\n",
      "Topic #3:\n",
      "newsread version tin cleveland freenet van pl8 la reserv bnr tor chi que pit case organ bo det pl9 host\n",
      "Topic #4:\n",
      "imag color pictur jpeg gif qualiti compress bit display ticket rpi trial peopl format time edu viewer tank ifa gnv\n",
      "Topic #5:\n",
      "game year team season player time basebal fan leagu point play run win organ edu way flyer sport home seri\n",
      "Topic #6:\n",
      "ax max g9v b8f a86 pl giz bhj wm bxn gk sl bj qax m3 okz lj biz ql b8g\n",
      "Topic #7:\n",
      "cx w7 hz ah chz mv lk ck scx t7 pl a86 mc sc air sq w1 a7 b8 k8\n",
      "Topic #8:\n",
      "god peopl thing religion question life christian way evid time point word argument reason someth truth exist fact claim exampl\n",
      "Topic #9:\n",
      "bike water ga motorcycl dod air light organ ride thing cycl articl plant lock tower tank power fuel convex temperatur\n",
      "Topic #10:\n",
      "player hockey play goal period pt vs team canada wing power buffalo island cup divis st playoff shot ranger jet\n",
      "Topic #11:\n",
      "entri output file program insur section rule health col char year int stream remark number check printf size use info\n",
      "Topic #12:\n",
      "jesu god church christian peopl day christ sin lord john time children way word son thing rutger scriptur paul faith\n",
      "Topic #13:\n",
      "chip encrypt secur key govern clipper technolog phone agenc law escrow devic privaci enforc data administr commun inform use block\n",
      "Topic #14:\n",
      "edu organ articl berkeley host pitt caltech bank com cs sgi moral harvard comput keith mellon pa duke pittsburgh cmu\n",
      "Topic #15:\n",
      "mit server applic function client widget sourc edu version window resourc display use com motif faq inform problem tar export\n",
      "Topic #16:\n",
      "db cs al hole cornel mov bh arizona si pop push bit seizur particl di comm byte et corn bl\n",
      "Topic #17:\n",
      "power wire ground circuit radio use cabl batteri input box electron suppli channel panel outlet signal voltag frequenc devic output\n",
      "Topic #18:\n",
      "sale price organ offer mail phone distribut condit box servic ship comput product compani year busi pleas purchas trade host\n",
      "Topic #19:\n",
      "bit key number valu messag unit time ripem cipher captain attack pgp way psu frank cryptographi secur session random patent\n",
      "Topic #20:\n",
      "card problem appl driver mac bit monitor memori video board machin port organ mous chip use mode color printer comput\n",
      "Topic #21:\n",
      "organ univers host edu thank anyon distribut nntp world pleas state engin comput advanc repli softwar hi mail keyword help\n",
      "Topic #22:\n",
      "com articl organ host sun world apr repli distribut jim write att gmt edu ti access mot nntp opinion messag\n",
      "Topic #23:\n",
      "food diseas problem doctor scienc patient studi effect time year articl peopl treatment theori medicin health rochest test research organ\n",
      "Topic #24:\n",
      "book page refer copi paper issu number edit translat time inform cover author date ad send letter magazin stori text\n",
      "Topic #25:\n",
      "peopl time day someth thing year home hour way wife someon children eye night dog hand everyth women woman stori\n",
      "Topic #26:\n",
      "jew war israel state peac land attack polici arab world articl time research countri question mcgill peopl right soldier citizen\n",
      "Topic #27:\n",
      "space orbit mission gov year earth launch flight moon satellit jpl rocket project cost data henri research time station program\n",
      "Topic #28:\n",
      "com articl hp organ ibm netcom austin edu stephanopoulo host mark write corpor phil vnet repli mitr vote packard org\n",
      "Topic #29:\n",
      "graphic softwar data imag program packag version code ftp comput pub format mail sourc tool inform fax contact site pc\n",
      "Topic #30:\n",
      "peopl armenian men turk virginia armenia world histori women villag articl govern turkey homosexu popul today serdar number cramer way\n",
      "Topic #31:\n",
      "gov organ research columbia center articl technolog host cc institut edu distribut gatech org internet bitnet repli tv write laboratori\n",
      "Topic #32:\n",
      "mail group access post commun servic list inform network internet news request organ user messag address peopl comput discuss time\n",
      "Topic #33:\n",
      "mk mu mr mo mj mp mm m5 mh md mi rz mt rl m3 tu m8 mx mz tt\n",
      "Topic #34:\n",
      "gun drug rate crime control firearm safeti peopl polic number time handgun year crimin state detector thing use problem cop\n",
      "Topic #35:\n",
      "law state court weapon case stratu right person amend author crime death polic com govern sw properti order agent evid\n",
      "Topic #36:\n",
      "presid mr program job state year univers press hous confer work meet offic school group senat time member administr educ\n",
      "Topic #37:\n",
      "peopl govern money thing countri right tax state way articl year time clinton power problem reason arm support case organ\n",
      "Topic #38:\n",
      "drive disk control scsi cd keyboard pc bu data mb rom bio interfac card problem devic jumper transfer cabl hd\n",
      "Topic #39:\n",
      "car engin road speed time oil year driver model mile thing tire auto problem myer vehicl way organ wheel ford\n",
      "打印主题-词语分布矩阵:\n",
      "[[ 0.025  0.025  0.025 ...,  0.025  0.025  0.025]\n",
      " [ 0.025  0.025  0.025 ...,  0.025  0.025  0.025]\n",
      " [ 0.025  0.025  0.025 ...,  0.025  0.025  0.025]\n",
      " ..., \n",
      " [ 0.025  0.025  0.025 ...,  0.025  0.025  0.025]\n",
      " [ 0.025  0.025  0.025 ...,  0.025  0.025  0.025]\n",
      " [ 0.025  0.025  0.025 ...,  0.025  0.025  0.025]]\n"
     ]
    }
   ],
   "source": [
    "#得到存储的tf_vectorizer,节省预处理时间\n",
    "lda = joblib.load(lda_path)\n",
    "# 输出结果\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    print('打印每个主题下权重较高的term:')\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print('打印主题-词语分布矩阵:')\n",
    "    print (model.components_)\n",
    "\n",
    "n_top_words=20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.88705234e-05   6.88705234e-05   6.88705234e-05 ...,   6.88705234e-05\n",
      "    6.88705234e-05   6.88705234e-05]\n",
      " [  3.08641975e-04   3.08641975e-04   3.08641975e-04 ...,   3.08641975e-04\n",
      "    3.63102090e-02   3.08641975e-04]\n",
      " [  1.04166667e-03   1.04166667e-03   1.04166667e-03 ...,   1.04166667e-03\n",
      "    1.04166667e-03   1.04166667e-03]\n",
      " ..., \n",
      " [  4.54545455e-04   4.54545455e-04   4.54545455e-04 ...,   4.54545455e-04\n",
      "    4.54545455e-04   4.88116809e-01]\n",
      " [  1.66666667e-03   1.66666667e-03   1.66666667e-03 ...,   1.66666667e-03\n",
      "    1.66666667e-03   1.66666667e-03]\n",
      " [  8.33333333e-04   8.33333333e-04   8.33333333e-04 ...,   8.33333333e-04\n",
      "    1.43931073e-01   8.33333333e-04]]\n",
      "训练集特征大小： (11314, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['train.set']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_path='train.set'\n",
    "train_feature = lda.transform(tf)\n",
    "print(train_feature)\n",
    "print('训练集特征大小：',train_feature.shape)\n",
    "\n",
    "train_set=[train_feature,train_label]\n",
    "#print(train_set)\n",
    "#训练集特征的保存/ 加载\n",
    "joblib.dump(train_set,train_set_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.88705234e-05   6.88705234e-05   6.88705234e-05 ...,   6.88705234e-05\n",
      "    6.88705234e-05   6.88705234e-05]\n",
      " [  3.08641975e-04   3.08641975e-04   3.08641975e-04 ...,   3.08641975e-04\n",
      "    3.63102090e-02   3.08641975e-04]\n",
      " [  1.04166667e-03   1.04166667e-03   1.04166667e-03 ...,   1.04166667e-03\n",
      "    1.04166667e-03   1.04166667e-03]\n",
      " ..., \n",
      " [  4.54545455e-04   4.54545455e-04   4.54545455e-04 ...,   4.54545455e-04\n",
      "    4.54545455e-04   4.88116809e-01]\n",
      " [  1.66666667e-03   1.66666667e-03   1.66666667e-03 ...,   1.66666667e-03\n",
      "    1.66666667e-03   1.66666667e-03]\n",
      " [  8.33333333e-04   8.33333333e-04   8.33333333e-04 ...,   8.33333333e-04\n",
      "    1.43931073e-01   8.33333333e-04]]\n",
      "训练集特征大小： (11314, 40)\n",
      "训练标签总数： 11314\n"
     ]
    }
   ],
   "source": [
    "#得到存储的train_set,节省预处理时间\n",
    "train_set_path='train.set'\n",
    "train_set = joblib.load(train_set_path)\n",
    "[train_feature,train_label]=train_set\n",
    "print(train_feature)\n",
    "print('训练集特征大小：',train_feature.shape)\n",
    "print('训练标签总数：',len(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取测试集特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试文档总数： 7532\n",
      "测试标签总数： 7532\n"
     ]
    }
   ],
   "source": [
    "#将所有文件名读入\n",
    "test_doc_names=[]\n",
    "test_datas=[]\n",
    "test_label_names=os.listdir(data_test_path)\n",
    "test_label=[]\n",
    "for i,label_name in enumerate(test_label_names):\n",
    "    label_path=os.path.join(data_test_path,label_name)\n",
    "    label_doc_names=os.listdir(label_path)\n",
    "    test_label.extend([i]*len(label_doc_names))\n",
    "    for label_doc_name in label_doc_names:\n",
    "        label_doc_path=os.path.join(label_path,label_doc_name)\n",
    "        test_doc_names.append(label_doc_path)\n",
    "        with open(label_doc_path,'r',encoding=\"ISO-8859-1\") as f:\n",
    "            test_datas.append(f.read())\n",
    "#print(test_doc_names)\n",
    "print('测试文档总数：',len(test_doc_names))\n",
    "print('测试标签总数：',len(test_label))\n",
    "#print(test_datas[0])\n",
    "#真正读取文档数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "300\n",
      "600\n",
      "900\n",
      "1200\n",
      "1500\n",
      "1800\n",
      "2100\n",
      "2400\n",
      "2700\n",
      "3000\n",
      "3300\n",
      "3600\n",
      "3900\n",
      "4200\n",
      "4500\n",
      "4800\n",
      "5100\n",
      "5400\n",
      "5700\n",
      "6000\n",
      "6300\n",
      "6600\n",
      "6900\n",
      "7200\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "test_textPre_FilePath='test_textPre.txt'\n",
    "def textPrecessing(text):\n",
    "    #小写化\n",
    "    text = text.lower()\n",
    "    #去除特殊标点\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, ' ')\n",
    "    #分词\n",
    "    wordLst = nltk.word_tokenize(text)\n",
    "    #去除停用词\n",
    "    filtered = [w for w in wordLst if w not in stopwords.words('english')]\n",
    "    #仅保留名词或特定POS   \n",
    "    refiltered =nltk.pos_tag(filtered)\n",
    "    filtered = [w for w, pos in refiltered if pos.startswith('NN')]\n",
    "    #词干化\n",
    "    ps = PorterStemmer()\n",
    "    filtered = [ps.stem(w) for w in filtered]\n",
    "\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "test_datas_clean = []\n",
    "for i,desc in enumerate(test_datas) :\n",
    "    test_datas_clean.append(textPrecessing(desc))\n",
    "    if i%300==0:\n",
    "        print(i)\n",
    "\n",
    "with open(test_textPre_FilePath, 'w') as f:\n",
    "    for line in test_datas_clean:\n",
    "        f.write(line+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earthquak predict organ cypress beaverton line articl atho rutger ingr write everyth heart citi portland countri state earthquak pass surpris surpris sleep mistak area fear area god misrepres god word word peopl word prophet bridg god suppos someon death glori power cloud lifetim year event question propheci make speaker prophet speak someth doubt condit relationship lord receiv propheci shadow doubt possibl imagin recognit wile follow way thing shadow doubt peopl check opinion realiti belief evid evid virtu ie faith luck result time christian exampl honesti backdrop lord prayer christian dan demonstr honesti start propheci conclud speaker\n"
     ]
    }
   ],
   "source": [
    "#读取预处理后的测试文本\n",
    "test_textPre_FilePath='test_textPre.txt'\n",
    "test_datas_clean = []\n",
    "with open(test_textPre_FilePath, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line != '':\n",
    "            test_datas_clean.append(' '.join(line.strip().split()[7:-1]))\n",
    "print(test_datas_clean[3])\n",
    "\n",
    "#读取tf_vectorizer，并对测试文本进行词频统计\n",
    "tf_ModelPath='tf.model'\n",
    "#得到存储的tf_vectorizer,节省预处理时间\n",
    "tf_vectorizer = joblib.load(tf_ModelPath)\n",
    "tf_test = tf_vectorizer.transform(test_datas_clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.16666667e-04   4.16666667e-04   4.16666667e-04 ...,   4.16666667e-04\n",
      "    4.16666667e-04   4.16666667e-04]\n",
      " [  3.33333333e-04   3.33333333e-04   3.33333333e-04 ...,   3.33333333e-04\n",
      "    1.66174794e-02   3.33333333e-04]\n",
      " [  6.09756098e-04   6.09756098e-04   6.09756098e-04 ...,   6.09756098e-04\n",
      "    6.09756098e-04   6.09756098e-04]\n",
      " ..., \n",
      " [  2.67202279e-02   2.93225645e-02   2.99643587e-02 ...,   2.11864407e-04\n",
      "    2.11864407e-04   2.11864407e-04]\n",
      " [  2.55203207e-01   8.92857143e-04   1.43786483e-01 ...,   1.21120159e-01\n",
      "    1.01320087e-01   8.92857143e-04]\n",
      " [  1.00000000e-03   1.00000000e-03   1.00000000e-03 ...,   5.39073133e-02\n",
      "    1.00000000e-03   1.09744794e-01]]\n",
      "测试集特征大小： (7532, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test.set']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_path='test.set'\n",
    "test_feature = lda.transform(tf_test)\n",
    "print(test_feature)\n",
    "print('测试集特征大小：',test_feature.shape)\n",
    "\n",
    "test_set=[test_feature,test_label]\n",
    "#print(train_set)\n",
    "#测试集特征的保存/ 加载\n",
    "joblib.dump(test_set,test_set_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.16666667e-04   4.16666667e-04   4.16666667e-04 ...,   4.16666667e-04\n",
      "    4.16666667e-04   4.16666667e-04]\n",
      " [  3.33333333e-04   3.33333333e-04   3.33333333e-04 ...,   3.33333333e-04\n",
      "    1.66174794e-02   3.33333333e-04]\n",
      " [  6.09756098e-04   6.09756098e-04   6.09756098e-04 ...,   6.09756098e-04\n",
      "    6.09756098e-04   6.09756098e-04]\n",
      " ..., \n",
      " [  2.67202279e-02   2.93225645e-02   2.99643587e-02 ...,   2.11864407e-04\n",
      "    2.11864407e-04   2.11864407e-04]\n",
      " [  2.55203207e-01   8.92857143e-04   1.43786483e-01 ...,   1.21120159e-01\n",
      "    1.01320087e-01   8.92857143e-04]\n",
      " [  1.00000000e-03   1.00000000e-03   1.00000000e-03 ...,   5.39073133e-02\n",
      "    1.00000000e-03   1.09744794e-01]]\n",
      "测试集特征大小： (7532, 40)\n",
      "测试标签总数： 7532\n"
     ]
    }
   ],
   "source": [
    "#得到存储的test_set,节省预处理时间\n",
    "test_set_path='test.set'\n",
    "test_set = joblib.load(test_set_path)\n",
    "[test_feature,test_label]=test_set\n",
    "print(test_feature)\n",
    "print('测试集特征大小：',test_feature.shape)\n",
    "print('测试标签总数：',len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集和测试集均处理完毕，开始分类器得训练，分类器选择SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_c(x_train, x_test, y_train, y_test):\n",
    "    # rbf核函数，设置数据权重\n",
    "    svc = SVC(kernel='rbf', class_weight='balanced',)\n",
    "    c_range = np.logspace(-5, 15, 11, base=2)\n",
    "    gamma_range = np.logspace(-9, 3, 13, base=2)\n",
    "    # 网格搜索交叉验证的参数范围，cv=3,3折交叉\n",
    "    param_grid = [{'kernel': ['rbf'], 'C': c_range, 'gamma': gamma_range}]\n",
    "    grid = GridSearchCV(svc, param_grid, cv=3, n_jobs=-1)\n",
    "    # 训练模型\n",
    "    clf = grid.fit(x_train, y_train)\n",
    "    # 计算测试集精度\n",
    "    score = clf.score(x_test, y_test)\n",
    "    print('精度为%s' % score)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精度为0.586165693043\n"
     ]
    }
   ],
   "source": [
    "clf=svm_c(train_feature,test_feature,train_label,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf.model']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存分类器\n",
    "clf_Path='clf.model'\n",
    "joblib.dump(clf,clf_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586165693043\n"
     ]
    }
   ],
   "source": [
    "score=clf.score(test_feature, test_label)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760738907548\n"
     ]
    }
   ],
   "source": [
    "score=clf.score(train_feature, train_label)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
